{
    "version": "1.0.0",
    "name": "appmixer.openai.core.createChatCompletion",
    "author": "Appmixer <info@appmixer.com>",
    "description": "Creates a model response for the given chat conversation.",
    "private": false,
    "quota": {},
    "inPorts": [
        {
            "name": "in",
            "schema": {
                "type": "object",
                "required": [
                    "model",
                    "messages"
                ],
                "properties": {
                    "model": {
                        "type": "string"
                    },
                    "messages": {
                        "type": "object",
                        "properties": {
                            "role": {
                                "type": "string"
                            },
                            "content": {
                                "type": "string"
                            }
                        }
                    },
                    "max_tokens": {
                        "type": "number"
                    },
                    "temperature": {
                        "type": "number"
                    },
                    "top_p": {
                        "type": "number"
                    },
                    "n": {
                        "type": "number"
                    },
                    "frequency_penalty": {
                        "type": "number"
                    },
                    "presence_penalty": {
                        "type": "number"
                    },
                    "seed": {
                        "type": "number"
                    },
                    "stop": {
                        "type": "object",
                        "properties": {
                            "sequence": {
                                "type": "string"
                            }
                        }
                    }
                }
            },
            "inspector": {
                "inputs": {
                    "model": {
                        "type": "select",
                        "index": 0,
                        "label": "Model",
                        "tooltip": "ID of the model to use. See the <a href=\"https://platform.openai.com/docs/models/model-endpoint-compatibility\" rel=\"noopener noreferrer\" target=\"_blank\">model endpoint compatibility</a> table for details on which models work with the Chat API. Example: gpt-3.5-turbo",
                        "source": {
                            "url": "/component/appmixer/openai/core/listModels?outPort=out",
                            "data": {
                                "transform": "./transform#toSelectOptions"
                            }
                        }
                    },
                    "messages": {
                        "type": "expression",
                        "label": "Messages",
                        "levels": [
                            "ADD"
                        ],
                        "index": 1,
                        "fields": {
                            "role": {
                                "type": "select",
                                "index": 0,
                                "label": "Role",
                                "options": [
                                    {
                                        "label": "Developer/System",
                                        "value": "system"
                                    },
                                    {
                                        "label": "User",
                                        "value": "user"
                                    },
                                    {
                                        "label": "Assistant",
                                        "value": "assistant"
                                    }
                                ]
                            },
                            "content": {
                                "type": "text",
                                "label": "Text content",
                                "index": 1
                            }
                        }
                    },
                    "max_tokens": {
                        "type": "number",
                        "index": 2,
                        "label": "Max Tokens",
                        "defaultValue": 2048,
                        "tooltip": "The maximum number of <a href=\"https://platform.openai.com/tokenizer\" rel=\"noopener noreferrer\" target=\"_blank\">tokens</a> to generate in the chat completion.</p>\n<p>The total length of input tokens and generated tokens is limited by the model's context length. <a href=\"https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\" rel=\"noopener noreferrer\" target=\"_blank\">Example Python code</a> for counting tokens."
                    },
                    "temperature": {
                        "type": "number",
                        "index": 3,
                        "label": "Temperature",
                        "tooltip": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or <code>top_p</code> but not both. Example: 1",
                        "defaultValue": 1,
                        "group": "advanced"
                    },
                    "top_p": {
                        "type": "number",
                        "index": 4,
                        "label": "Top P",
                        "tooltip": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or <code>temperature</code> but not both. Example: 1",
                        "defaultValue": 1,
                        "group": "advanced"
                    },
                    "n": {
                        "type": "number",
                        "index": 5,
                        "label": "Number",
                        "tooltip": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep <code>n</code> as <code>1</code> to minimize costs. Example: 1",
                        "defaultValue": 1,
                        "min": 1,
                        "group": "advanced"
                    },
                    "frequency_penalty": {
                        "type": "number",
                        "index": 6,
                        "label": "Frequency Penalty",
                        "tooltip": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.<a href=\"https://platform.openai.com/docs/guides/text-generation/parameter-details\" rel=\"noopener noreferrer\" target=\"_blank\">See more information about frequency and presence penalties.</a>",
                        "min": -2,
                        "max": 2,
                        "group": "advanced"
                    },
                    "presence_penalty": {
                        "type": "number",
                        "index": 7,
                        "label": "Presence Penalty",
                        "tooltip": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.<a href=\"https://platform.openai.com/docs/guides/text-generation/parameter-details\" rel=\"noopener noreferrer\" target=\"_blank\">See more information about frequency and presence penalties.",
                        "defaultValue": 0,
                        "min": -2,
                        "max": 2,
                        "group": "advanced"
                    },
                    "seed": {
                        "type": "number",
                        "index": 8,
                        "label": "Seed",
                        "tooltip": "This feature is in Beta. \nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same <code>seed</code> and parameters should return the same result. Determinism is not guaranteed, and you should refer to the <code>system_fingerprint</code> response parameter to monitor changes in the backend.",
                        "group": "advanced"
                    },
                    "stop": {
                        "type": "expression",
                        "label": "Stop sequence",
                        "tooltip": "Maximum: 4",
                        "levels": [
                            "ADD"
                        ],
                        "index": 9,
                        "group": "advanced",
                        "fields": {
                            "sequence": {
                                "type": "text",
                                "label": "Stop sequence text",
                                "index": 1
                            }
                        }
                    }
                },
                "groups": {
                    "advanced": {
                        "label": "Advanced",
                        "index": 1
                    }
                }
            }
        }
    ],
    "outPorts": [
        {
            "name": "out",
            "options": [
                {
                    "label": "Id",
                    "value": "id"
                },
                {
                    "label": "Choices",
                    "value": "choices",
                    "schema": {
                        "type": "array",
                        "description": "A list of chat completion choices. Can be more than one if `n` is greater than 1.",
                        "items": {
                            "type": "object",
                            "required": [
                                "finish_reason",
                                "index",
                                "message"
                            ],
                            "properties": {
                                "finish_reason": {
                                    "type": "string",
                                    "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,\n`length` if the maximum number of tokens specified in the request was reached,\n`content_filter` if content was omitted due to a flag from our content filters,\n`tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.\n",
                                    "enum": [
                                        "stop",
                                        "length",
                                        "tool_calls",
                                        "content_filter",
                                        "function_call"
                                    ]
                                },
                                "index": {
                                    "type": "integer",
                                    "description": "The index of the choice in the list of choices."
                                },
                                "message": {
                                    "type": "object",
                                    "description": "A chat completion message generated by the model.",
                                    "required": [
                                        "role",
                                        "content"
                                    ],
                                    "properties": {
                                        "content": {
                                            "type": "string",
                                            "description": "The contents of the message.",
                                            "nullable": true
                                        },
                                        "tool_calls": {
                                            "type": "array",
                                            "description": "The tool calls generated by the model, such as function calls.",
                                            "items": {
                                                "type": "object",
                                                "required": [
                                                    "id",
                                                    "type",
                                                    "function"
                                                ],
                                                "properties": {
                                                    "id": {
                                                        "type": "string",
                                                        "description": "The ID of the tool call."
                                                    },
                                                    "type": {
                                                        "type": "string",
                                                        "enum": [
                                                            "function"
                                                        ],
                                                        "description": "The type of the tool. Currently, only `function` is supported."
                                                    },
                                                    "function": {
                                                        "type": "object",
                                                        "description": "The function that the model called.",
                                                        "required": [
                                                            "name",
                                                            "arguments"
                                                        ],
                                                        "properties": {
                                                            "name": {
                                                                "type": "string",
                                                                "description": "The name of the function to call."
                                                            },
                                                            "arguments": {
                                                                "type": "string",
                                                                "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        },
                                        "role": {
                                            "type": "string",
                                            "enum": [
                                                "assistant"
                                            ],
                                            "description": "The role of the author of this message."
                                        },
                                        "function_call": {
                                            "type": "object",
                                            "deprecated": true,
                                            "description": "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.",
                                            "required": [
                                                "name",
                                                "arguments"
                                            ],
                                            "properties": {
                                                "arguments": {
                                                    "type": "string",
                                                    "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
                                                },
                                                "name": {
                                                    "type": "string",
                                                    "description": "The name of the function to call."
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                {
                    "label": "Created",
                    "value": "created"
                },
                {
                    "label": "Model",
                    "value": "model"
                },
                {
                    "label": "System Fingerprint",
                    "value": "system_fingerprint"
                },
                {
                    "label": "Object",
                    "value": "object"
                },
                {
                    "label": "Usage",
                    "value": "usage"
                },
                {
                    "label": "Usage Completion Tokens",
                    "value": "usage.completion_tokens"
                },
                {
                    "label": "Usage Prompt Tokens",
                    "value": "usage.prompt_tokens"
                },
                {
                    "label": "Usage Total Tokens",
                    "value": "usage.total_tokens"
                }
            ]
        }
    ],
    "properties": {},
    "auth": {
        "service": "appmixer:openai"
    },
    "icon": "data:image/svg+xml;base64,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"
}
