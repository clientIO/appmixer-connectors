{
    "version": "1.0.0",
    "name": "appmixer.openai.core.createCompletion",
    "author": "Appmixer <info@appmixer.com>",
    "description": "<label>Creates a completion for the provided prompt and parameters.</label>",
    "private": false,
    "quota": {},
    "inPorts": [
        {
            "name": "in",
            "schema": {
                "type": "object",
                "required": [
                    "model",
                    "prompt",
                    "max_tokens"
                ],
                "properties": {
                    "model": {
                        "type": "string"
                    },
                    "prompt": {
                        "type": "string"
                    },
                    "max_tokens": {
                        "type": "number"
                    },
                    "temperature": {
                        "type": "number"
                    },
                    "top_p": {
                        "type": "number"
                    },
                    "n": {
                        "type": "number"
                    },
                    "logprobs": {
                        "type": "number"
                    },
                    "echo": {
                        "type": "boolean"
                    },
                    "stop": {
                        "type": "string"
                    },
                    "presence_penalty": {
                        "type": "number"
                    },
                    "frequency_penalty": {
                        "type": "number"
                    },
                    "best_of": {
                        "type": "number"
                    },
                    "user": {
                        "type": "string"
                    },
                    "seed": {
                        "type": "number"
                    },
                    "suffix": {
                        "type": "string"
                    }
                }
            },
            "inspector": {
                "inputs": {
                    "model": {
                        "type": "text",
                        "index": 0,
                        "label": "Model",
                        "tooltip": "The ID of the model to use for this request",
                        "source": {
                            "url": "/component/appmixer/openai/core/listModels?outPort=out",
                            "data": {
                                "messages": {
                                    "in/xConnectorOutputType": "array"
                                },
                                "transform": "./transform#toSelectOptions"
                            }
                        }
                    },
                    "prompt": {
                        "type": "text",
                        "index": 1,
                        "label": "Prompt",
                        "tooltip": "Example: This is a test."
                    },
                    "max_tokens": {
                        "type": "number",
                        "index": 2,
                        "label": "Max Tokens",
                        "tooltip": "The maximum number of <a href=\"https://platform.openai.com/tokenizer\" rel=\"noopener noreferrer\" target=\"_blank\">tokens</a> to generate in the completion.\nThe token count of your prompt plus <code>max_tokens</code> cannot exceed the model's context length. <a href=\"https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\" rel=\"noopener noreferrer\" target=\"_blank\">Example Python code</a> for counting tokens.\n Example: 16",
                        "defaultValue": 16,
                        "min": 0
                    },
                    "temperature": {
                        "type": "number",
                        "index": 3,
                        "label": "Temperature",
                        "tooltip": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. OpenAI generally recommends altering this or <code>top_p</code> but not both. Example: 1",
                        "defaultValue": 1,
                        "min": 0,
                        "max": 2
                    },
                    "top_p": {
                        "type": "number",
                        "index": 4,
                        "label": "Top P",
                        "tooltip": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. OpenAI generally recommends altering this or <code>temperature</code> but not both. Example: 1",
                        "defaultValue": 1,
                        "min": 0,
                        "max": 1
                    },
                    "n": {
                        "type": "number",
                        "index": 6,
                        "label": "N",
                        "tooltip": "How many completions to generate for each prompt.\n<strong>Note:</strong> Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.\n Example: 1",
                        "defaultValue": 1,
                        "min": 1,
                        "max": 128
                    },
                    "logprobs": {
                        "type": "number",
                        "index": 7,
                        "label": "Logprobs",
                        "tooltip": "Include the log probabilities on the <code>logprobs</code> most likely tokens, as well the chosen tokens. For example, if <code>logprobs</code> is 5, the API will return a list of the 5 most likely tokens. The API will always return the <code>logprob</code> of the sampled token, so there may be up to <code>logprobs+1</code> elements in the response.\nThe maximum value for <code>logprobs</code> is 5.",
                        "min": 0,
                        "max": 5
                    },
                    "echo": {
                        "type": "toggle",
                        "index": 8,
                        "label": "Echo",
                        "tooltip": "If <strong>true</strong>, the API includes the input prompt in the response.",
                        "defaultValue": false
                    },
                    "stop": {
                        "type": "text",
                        "index": 9,
                        "label": "Stop",
                        "tooltip": "One or more sequences where the model should stop generating tokens."
                    },
                    "presence_penalty": {
                        "type": "number",
                        "index": 10,
                        "label": "Presence Penalty",
                        "tooltip": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n<a href=\"https://platform.openai.com/docs/guides/text-generation/parameter-details\" rel=\"noopener noreferrer\" target=\"_blank\">See more information about frequency and presence penalties.</a>",
                        "defaultValue": 0,
                        "min": -2,
                        "max": 2
                    },
                    "frequency_penalty": {
                        "type": "number",
                        "index": 11,
                        "label": "Frequency Penalty",
                        "tooltip": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n<a href=\"https://platform.openai.com/docs/guides/text-generation/parameter-details\" rel=\"noopener noreferrer\" target=\"_blank\">See more information about frequency and presence penalties.</a>",
                        "defaultValue": 0,
                        "min": -2,
                        "max": 2
                    },
                    "best_of": {
                        "type": "number",
                        "index": 12,
                        "label": "Best Of",
                        "tooltip": "Generates <code>best_of</code> completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\nWhen used with <code>n</code>, <code>best_of</code> controls the number of candidate completions and <code>n</code> specifies how many to return â€“ <code>best_of</code> must be greater than <code>n</code>.\n<strong>Note:</strong> Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for <code>max_tokens</code> and <code>stop</code>.",
                        "defaultValue": 1,
                        "min": 0,
                        "max": 20
                    },
                    "user": {
                        "type": "text",
                        "index": 13,
                        "label": "User",
                        "tooltip": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. <a href=\"https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids\" rel=\"noopener noreferrer\" target=\"_blank\">Learn more</a>.\n Example: user-1234"
                    },
                    "seed": {
                        "type": "number",
                        "index": 14,
                        "label": "Seed",
                        "tooltip": "If specified, the system will make a best effort to sample deterministically, such that repeated requests with the same <strong>seed</strong> and parameters should return the same result. Determinism is not guaranteed, and you should refer to the <strong>system_fingerprint</strong> response parameter to monitor changes in the backend."
                    },
                    "suffix": {
                        "type": "text",
                        "index": 15,
                        "label": "Suffix",
                        "tooltip": "The suffix that comes after a completion of inserted text. This is only supported for <strong>Model</strong> gpt-3.5-turbo-instruct."
                    }
                }
            }
        }
    ],
    "outPorts": [
        {
            "name": "out",
            "options": [
                {
                    "label": "Id",
                    "value": "id"
                },
                {
                    "label": "Choices",
                    "value": "choices",
                    "schema": {
                        "type": "array",
                        "description": "The list of completion choices the model generated for the input prompt.",
                        "items": {
                            "type": "object",
                            "required": [
                                "finish_reason",
                                "index",
                                "logprobs",
                                "text"
                            ],
                            "properties": {
                                "finish_reason": {
                                    "type": "string",
                                    "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,\n`length` if the maximum number of tokens specified in the request was reached,\nor `content_filter` if content was omitted due to a flag from our content filters.\n",
                                    "enum": [
                                        "stop",
                                        "length",
                                        "content_filter"
                                    ]
                                },
                                "index": {
                                    "type": "integer"
                                },
                                "logprobs": {
                                    "type": "object",
                                    "nullable": true,
                                    "properties": {
                                        "text_offset": {
                                            "type": "array",
                                            "items": {
                                                "type": "integer"
                                            }
                                        },
                                        "token_logprobs": {
                                            "type": "array",
                                            "items": {
                                                "type": "number"
                                            }
                                        },
                                        "tokens": {
                                            "type": "array",
                                            "items": {
                                                "type": "string"
                                            }
                                        },
                                        "top_logprobs": {
                                            "type": "array",
                                            "items": {
                                                "type": "object",
                                                "additionalProperties": {
                                                    "type": "number"
                                                }
                                            }
                                        }
                                    }
                                },
                                "text": {
                                    "type": "string"
                                }
                            }
                        }
                    }
                },
                {
                    "label": "Created",
                    "value": "created"
                },
                {
                    "label": "Model",
                    "value": "model"
                },
                {
                    "label": "System Fingerprint",
                    "value": "system_fingerprint"
                },
                {
                    "label": "Object",
                    "value": "object"
                },
                {
                    "label": "Usage",
                    "value": "usage"
                },
                {
                    "label": "Usage Completion Tokens",
                    "value": "usage.completion_tokens"
                },
                {
                    "label": "Usage Prompt Tokens",
                    "value": "usage.prompt_tokens"
                },
                {
                    "label": "Usage Total Tokens",
                    "value": "usage.total_tokens"
                }
            ]
        }
    ],
    "auth": {
        "service": "appmixer:openai"
    },
    "icon": "data:image/svg+xml;base64,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"
}
